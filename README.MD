# ğŸ“š RAG PDF Chat

Minimal Retrieval-Augmented Generation (RAG) application for querying PDFs using a local LLM and vector search.

---

## ğŸš€ Features

- PDF ingestion & chunking  
- HuggingFace embeddings (all-MiniLM-L6-v2)  
- Chroma persistent vector database  
- Similarity-based retrieval (Top-K)  
- FLAN-T5 local LLM generation  
- Strict grounded QA prompt (hallucination control)  
- FastAPI backend  
- Simple HTML frontend  
- Pytest-ready  

---

## ğŸ— Architecture

Pipeline:

PDF â†’ Chunk â†’ Embed â†’ Store â†’ Retrieve â†’ Prompt â†’ Generate â†’ Answer

Core Modules:

- `main.py` â€“ API layer  
- `ingestion.py` â€“ PDF processing  
- `vector_store.py` â€“ Chroma + embeddings  
- `rag.py` â€“ Retrieval orchestration  
- `llm.py` â€“ FLAN-T5 generation  
- `prompts.py` â€“ QA prompt control  
- `config.py` â€“ Model & chunk configuration  

---

## ğŸ“¦ Tech Stack

Backend:
- FastAPI  
- LangChain  
- ChromaDB  
- HuggingFace Transformers  
- Sentence Transformers  
- PyTorch  

Frontend:
- HTML + JavaScript  

Testing:
- Pytest  

---

## âš™ Configuration

Defined in `config.py`:

- `MODEL_NAME`
- `CHUNK_SIZE`
- `CHUNK_OVERLAP`
- `TOP_K`
- `PERSIST_DIR`

Default model:
google/flan-t5-small


---

## ğŸ“‚ Project Structure

.
â”œâ”€â”€ main.py
â”œâ”€â”€ ingestion.py
â”œâ”€â”€ rag.py
â”œâ”€â”€ vector_store.py
â”œâ”€â”€ llm.py
â”œâ”€â”€ prompts.py
â”œâ”€â”€ schemas.py
â”œâ”€â”€ config.py
â”œâ”€â”€ helper/
â”‚ â””â”€â”€ clean_text.py
â”œâ”€â”€ index.html
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ vector_db/
â”œâ”€â”€ uploaded_pdfs/
â”œâ”€â”€ retrieved_chunks.txt
â””â”€â”€ tests/


---

## ğŸ§ª Testing

Run:
pytest -v


---

## ğŸ” Notes

- Re-ingestion resets vector collection  
- Retrieved chunks logged to `retrieved_chunks.txt`  
- CORS currently open (`*`)  
- Fully local LLM (no external API required)  

---

## ğŸ¯ Purpose

A clean, modular RAG architecture suitable for:

- Local experimentation  
- LLM backend abstraction  
- CI/CD integration  
- Future cloud deployment  